<!DOCTYPE html>
<html lang="ru">
<head>

    <!-- HTML Meta Tags -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <title>Почему ИИ&nbsp;копирует наши предубеждения и&nbsp;как это исправить</title>
    <meta name="description" content="Полностью освободить себя от&nbsp;предрассудков мы&nbsp;вряд&nbsp;ли сумеем, а&nbsp;вот у&nbsp;ИИ больше шансов">
    
    <!-- Facebook Meta Tags -->
    <meta property="og:url" content="supernova.is">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Почему ИИ&nbsp;копирует наши предубеждения и&nbsp;как это исправить">
    <meta property="og:description" content="Полностью освободить себя от&nbsp;предрассудков мы&nbsp;вряд&nbsp;ли сумеем, а&nbsp;вот у&nbsp;ИИ больше шансов">
    <meta property="og:image" content="/img/ai-bias/og.jpg">
    
    <!-- Twitter Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="supernova.is">
    <meta property="twitter:url" content="supernova.is">
    <meta name="twitter:title" content="Почему ИИ&nbsp;копирует наши предубеждения и&nbsp;как это исправить">
    <meta name="twitter:description" content="Полностью освободить себя от&nbsp;предрассудков мы&nbsp;вряд&nbsp;ли сумеем, а&nbsp;вот у&nbsp;ИИ больше шансов">
    <meta name="twitter:image" content="/img/ai-bias/og.jpg">

    <!-- Meta Tags Generated via https://www.opengraph.xyz -->

    <!-- PLAUSABLE -->

    <script async defer data-domain="supernova.is" src="https://plausible.io/js/plausible.js"></script>

    <!-- FAVICONS -->

    <link rel="apple-touch-icon" sizes="180x180" href="/fav/apple-touch-icon.png?v=YA376Q73k9">
    <link rel="icon" type="image/png" sizes="32x32" href="/fav/favicon-32x32.png?v=YA376Q73k9">
    <link rel="icon" type="image/png" sizes="16x16" href="/fav/favicon-16x16.png?v=YA376Q73k9">
    <link rel="manifest" href="/fav/site.webmanifest?v=YA376Q73k9">
    <link rel="mask-icon" href="/fav/safari-pinned-tab.svg?v=YA376Q73k9" color="#000000">
    <link rel="shortcut icon" href="/fav/favicon.ico?v=YA376Q73k9">
    <meta name="msapplication-TileColor" content="#ffffff">

    <!-- STYLE -->

    <link rel="stylesheet" href="/style.css">

    <style>

        h2 {
            text-align: center;
            margin-left: auto;
            margin-right: auto;
        }

        h3 {
            margin-bottom: 0;
        }

    </style>
    
</head>
<body>
    <div class="content">
    <div class="logo">
        <a class="logo-link" href="/">
        <svg width="394" height="30vh" viewBox="0 0 394 292" fill="none" xmlns="http://www.w3.org/2000/svg">
            <style>
                @media (prefers-color-scheme: dark) {
                circle, rect {
                    fill: white;
                }
                    rect, line {
                    stroke: white;
                    }
                }
            </style>
            <circle cx="197.005" cy="253.667" r="38.1241" fill="black"/>
            <rect x="0.480469" y="245.831" width="393.039" height="15.6721" rx="7.83604" fill="black"/>
            <line x1="269.236" y1="172.369" x2="367.61" y2="65.0791" stroke="black" stroke-width="23.9522" stroke-linecap="round" stroke-dasharray="5.99 59.88"/>
            <line x1="11.9761" y1="-11.9761" x2="157.539" y2="-11.9761" transform="matrix(-0.675817 -0.737069 -0.737069 0.675817 128.565 189.29)" stroke="black" stroke-width="23.9522" stroke-linecap="round" stroke-dasharray="5.99 59.88"/>
            <line x1="198.509" y1="158.144" x2="198.508" y2="12.5809" stroke="black" stroke-width="23.9522" stroke-linecap="round" stroke-dasharray="5.99 59.88"/>
        </svg>
        </a>
    </div>
    <div class="middle-links">
        <a href="/about.html">Манифест</a>
        <a href="/us.html" style="margin-right: 0px !important">О нас</a>
    </div>
    <div class="headline-block">
        <figure class="image-container">
            <img style="aspect-ratio: 16/10;" src="https://media-cldnry.s-nbcnews.com/image/upload/t_fit-2000w,f_auto,q_auto:best/newscms/2019_11/2781986/190311-flickr-ibm-main-kh.gif" alt="">
            <figcaption class="caption" onclick="window.open('https://www.nbcnews.com/tech/internet/facial-recognition-s-dirty-little-secret-millions-online-photos-scraped-n981921','_blank');">Гифка: Erik Carter для NBC News. Фотографии: Greg Peverill-Conti</figcaption>
        </figure>
        <h2 class="headline pre-headline byline" rel="author" onclick="window.open('https://t.me/smlldt','_blank');">Денис Протопопов</h2>
        <h1 class="headline">Почему ИИ&nbsp;копирует наши предубеждения и&nbsp;как это исправить</h1>

        <time pubdate datetime="2021-10-05">5 октября 2021</time>
    </div>

    <article class="text" lang="ru">

        <em>Предубеждения&nbsp;&mdash; пугающая, но&nbsp;неизбежная часть нашей жизни. С&nbsp;одной стороны, технологии могут помочь в&nbsp;их&nbsp;преодолении: например, появляются онлайн-сообщества для персон и&nbsp;групп, подвергающихся дискриминации. С&nbsp;другой стороны, алгоритмы искусственного интеллекта могут транслировать определенные предрассудки и&nbsp;таким образом способствовать их&nbsp;закреплению и&nbsp;усугублению. Из-за чего алгоритм становится носителем предрассудков и&nbsp;дискриминационных практик и&nbsp;что мы&nbsp;можем сделать, чтобы строить более справедливые системы, объясняет художник и&nbsp;исследователь Денис Протопопов.</em>

        <h2>•</h2>

        <p>
        Алгоритмы легко масштабируются и&nbsp;позволяют автоматизировать большое количество рутинных, сложных и&nbsp;ответственных задач. При этом довольно высокая и&nbsp;часто удивляющая точность их&nbsp;работы создает ощущение объективности и&nbsp;достоверности. Отсюда логичным образом возникает соблазн передать искусственному интеллекту задачи, требующие рационального подхода. Например, машине пробуют поручить прием на&nbsp;работу или в&nbsp;университет, вынесение приговоров в&nbsp;суде, медицинскую диагностику и&nbsp;многое другое. Однако человеческие предубеждения, давно бытующие в&nbsp;обществе, даже в&nbsp;таком случае могут проявиться в&nbsp;решениях &laquo;объективного&raquo; алгоритма.
        </p>
        <p>
        Можно выделить три основных фактора, из-за которых&nbsp;ИИ приобретает человеческую склонность к&nbsp;предрассудкам:
        <ul style="margin-top: 0;">
        <li>входные данные, которые являются фундаментальной составляющей систем на&nbsp;основе искусственного интеллекта, 
        <li>архитектура алгоритмов,
        <li>способность алгоритмов находить неочевидные для человека признаки.
        </li>
        </ul>

        <h2>Входные данные: bias&nbsp;in, bias&nbsp;out</h2>

        <p>
        Способность самостоятельно решать задачи по&nbsp;мере накопления опыта&nbsp;&mdash; главная особенность и&nbsp;ценность алгоритмов искусственного интеллекта. Если мы&nbsp;хотим обучить алгоритм распознавать рукописный текст, нам нужно подготовить десятки тысяч примеров написания различных букв. Если мы&nbsp;хотим создать систему для распознавания спама, нам нужна большая коллекция спам-писем; алгоритм для решения о&nbsp;выдаче кредита&nbsp;&mdash; кредитную историю сотен тысяч людей. Алгоритм для предсказания преступлений&nbsp;&mdash; исторические данные о&nbsp;задержаниях, совершенных преступлениях и&nbsp;приговорах. Все алгоритмы на&nbsp;основе искусственного интеллекта обращаются к&nbsp;прошлому, чтобы предсказывать события, которые произойдут в&nbsp;будущем.
        </p>
        <p>
        ProPublica&nbsp;в&nbsp;своем расследовании &quot;<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias</a>&quot; (Машинные предрассудки) описывает практику применения автоматизированной системы COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) в&nbsp;различных штатах США. Алгоритм оценивает вероятность совершения подсудимым нового преступления в&nbsp;будущем, и&nbsp;как следует из&nbsp;ее&nbsp;названия, позволяет оценить возможность присвоения альтернативной меры пресечения. Судьи в&nbsp;некоторых штатах действительно <a href="https://www.iacajournal.org/articles/10.36745/ijca.343/">учитывают</a> оценки этой системы при вынесении окончательного&nbsp;решения.
        </p>
        <p>
        Проверяя, какие из&nbsp;прогнозов системы наиболее близки к&nbsp;истине, журналисты из&nbsp;ProPublica <a href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm">обнаружили</a>, что COMPAS предвзят в&nbsp;своем прогнозировании по&nbsp;отношению к&nbsp;темнокожим людям. Система ложно оценивала афроамериканских подсудимых как рецидивистов, а&nbsp;белых подсудимых считала менее склонными к&nbsp;повторным преступлениям. Такие прогнозы часто не&nbsp;имели ничего общего с&nbsp;уже состоявшейся реальностью.
        </p>
        <p>
        Предвзятость по&nbsp;отношению к&nbsp;темнокожим подсудимым со&nbsp;стороны алгоритма объясняется предвязостью, содержавшейся в&nbsp;данных, на&nbsp;основе которых ранее принимались решения. <a href="https://www.yalelawjournal.org/pdf/Mayson_p5g2tz2m.pdf">По&nbsp;словам</a> Сандры Мэйсон, профессора права из&nbsp;университета Пенсильвании, хотя такие системы как COMPAS были созданы, чтобы прогнозировать вероятные преступления, на&nbsp;самом деле они предсказывают <em>только факт ареста</em>. Так происходит, потому что данные, на&nbsp;которых строится система, почти не&nbsp;содержат информацию о&nbsp;совершенных преступлениях. К&nbsp;примеру, по&nbsp;некоторым делам люди были ложно обвинены; другие&nbsp;&mdash; не&nbsp;привели к&nbsp;заключению; часть арестов и&nbsp;вовсе были ошибочны.
        </p>
        <p>
        Статистика показывает: темнокожие люди в&nbsp;США оказываются под арестом чаще, чем белые люди. При этом количество преступлений, совершенных первыми и&nbsp;вторыми, примерно равно. Мэйсон ссылается на&nbsp;отчет <a href="https://www.aclu.org/report/report-war-marijuana-black-and-white?redirect=criminal-law-reform/war-marijuana-black-and-white">ACLU</a>, согласно которому и&nbsp;темнокожие, и&nbsp;белые люди употребляют марихуану более-менее в&nbsp;одинаковом объеме, однако именно первые чаще всего оказываются под арестом из-за этого.
        </p>
        <p>
        Искусственный интеллект на&nbsp;различных этапах судопроизводства используют не&nbsp;только в&nbsp;США. В&nbsp;<a href="https://www.iacajournal.org/articles/10.36745/ijca.367/">Китае</a> эта технология используется для оптимизации обработки и&nbsp;подготовки различных документов, стенографии и&nbsp;идентификации личности. В&nbsp;России ИИ&nbsp;тоже <a href="https://iz.ru/1183206/dmitrii-alekseev/avtomat-na-sluzhbe-zakona-sudy-privlekli-k-rabote-iskusstvennyi-intellekt">используется</a> для подготовки документов и&nbsp;помощи при формировании судебных приказов по&nbsp;взысканию денежных средств. А&nbsp;<a href="https://www.thelawyersdaily.ca/articles/11582/estonia-set-to-introduce-ai-judge-in-small-claims-court-to-clear-court-backlog-">эстонское</a> министерство юстиции заинтересовано в&nbsp;использовании автоматизированных судов для рассмотрения мелких исков до&nbsp;7&nbsp;000&nbsp;евро.
        </p>
        <p>
        Однако&nbsp;кейс с&nbsp;применением системы COMPAS, описанный ProPublica,&nbsp;&mdash; пожалуй, самый громкий пример опасного дизайна системы искусственного интеллекта для участия в&nbsp;судебных разбирательствах. На&nbsp;это расследование, например, ссылается Европейская этическая <a href="https://rm.coe.int/ethical-charter-en-for-publication-4-december-2018/16808f699c">хартия</a> об&nbsp;использовании искусственного интеллекта в&nbsp;судебных системах, принятая в&nbsp;2018 году Советом Европы. Она указывает на&nbsp;предрассудки, которые могут проявляться в&nbsp;системах&nbsp;ИИ: например, на&nbsp;тот факт, что история поведения группы людей, по&nbsp;сути, будет определять судьбу отдельно взятого человека.
        </p>
        <p>
        Есть&nbsp;интересный кейс и&nbsp;из&nbsp;российской практики: создатели проекта &laquo;<a href="https://readymag.com/u3045877410/algoritmsveta/">Алгоритм Света</a>&raquo; проанализировали тексты уголовных дел, связанных с&nbsp;домашним насилием. В&nbsp;итоге они создали <a href="https://github.com/LanaLob/algorithm_sveta">программу</a>, которая по&nbsp;тексту судебного приговора об&nbsp;убийстве женщины, определяла, подвергалась&nbsp;ли жертва домашнему насилию. При этом создатели &laquo;Алгоритма Света&raquo; поясняют, как такой механизм помогает оценить реальный масштаб проблемы: в&nbsp;официальной статистике о&nbsp;домашнем насилии указываются только люди, которые юридически являются членами семьи; в&nbsp;ней не&nbsp;учитываются партнеры, не&nbsp;состоящие в&nbsp;браке, бывшие супруги и&nbsp;т.&nbsp;д. Кроме того, в&nbsp;статистике содержатся только уголовные, а&nbsp;не&nbsp;административные&nbsp;дела. 
        </p>

        <figure class="image-container inline-image">
            <img src="https://d2w9rnfcy7mm78.cloudfront.net/7080831/original_3913e89eb36f6a7a5bcba6d700e0e0d3.gif?1588198891?bc=0" alt="">
            <figcaption class="caption" onclick="window.open('https://blog.x.company/introducing-tidal-1914257962c3','_blank');">Гифка: Tidal</figcaption>
        </figure>

        <h2>1000 и&nbsp;1&nbsp;категория</h2>

        <p>
        Упомянутые выше кейсы касаются только одной из&nbsp;проблем, связанных с&nbsp;некорректно подготовленными данными для алгоритмов искусственного интеллекта, и&nbsp;только одного случая применения технологии. Прямо сейчас где-то собираются и&nbsp;создаются наборы данных для обучения систем ИИ&nbsp;&mdash; которые впоследствии будут использоваться для большого числа самых разных задач. Такие системы впечатляюще успешно распознают объекты на&nbsp;картинках, лица людей и&nbsp;речь, генерируют тексты и&nbsp;делают много еще&nbsp;чего.
        </p>
        <p>
        Вот&nbsp;один из&nbsp;примеров. ImageNet&nbsp;&mdash; это большой датасет, содержащий 14&nbsp;миллионов изображений, относящихся к&nbsp;20&nbsp;тысячам категорий. Благодаря этому набору данных, улучшенным вычислительным способностям и&nbsp;подбору подходящего алгоритма в&nbsp;начале 2010-х случился прорыв в&nbsp;компьютерном зрении. Эта область искусственного интеллекта ставит своей целью создание машин, которые могут распознавать визуальные образы и&nbsp;генерировать новые изображения. Теперь алгоритмы умеют успешно распознавать и&nbsp;выделять один или несколько объектов на&nbsp;изображении. Создается ощущение, что проблема решена: компьютер способен определить, что перед ним находится. Однако исследователь Кейт Кроуфорд и&nbsp;художник Тревор Паглен в&nbsp;своем <a href="https://excavating.ai">эссе</a> утверждают, что задача научить компьютер описывать, что он&nbsp;видит перед собой, всегда будет сопряжена с&nbsp;рядом этических и&nbsp;политических проблем. И&nbsp;дело снова в&nbsp;данных.
        </p>
        <p>
        Кроуфорд&nbsp;и&nbsp;Паглен называют свой метод изучения датасетов <strong>&laquo;археологией&raquo;</strong>. Исследователи изучают содержимое баз данных и&nbsp;пытаются понять, как они устроены. Если&nbsp;мы, как и&nbsp;авторы, применим метод археологии к&nbsp;ImageNet, то&nbsp;увидим ничем не&nbsp;примечательные категории: различные виды транспорта, фруктов, ягод, мебели, природных явлений и&nbsp;тысячи других. Причем все категории сгруппированы иерархически: одна категория содержит несколько других и&nbsp;так далее. 
        </p>
        <p>
        Среди&nbsp;описаний людей, которые выделяют авторы <a href="https://excavating.ai">эссе</a>, в&nbsp;датасете можно найти: &laquo;мужчина&raquo;, &laquo;женщина&raquo;, &laquo;отец&raquo;, &laquo;бисексуал&raquo;, &laquo;бойскаут&raquo;, &laquo;гермафродит&raquo;, &laquo;наркоман&raquo;, &laquo;парикмахер&raquo;, &laquo;алкоголик&raquo;, &laquo;шизофреник&raquo;, &laquo;слабак&raquo;, &laquo;нейробиолог&raquo;, &laquo;проститутка&raquo;, &laquo;неудачник&raquo;, &laquo;деревенщина&raquo;, &laquo;мулат&raquo;, &laquo;большевик&raquo;, &laquo;антисемит&raquo; и&nbsp;десятки других. Причем каждой из&nbsp;категорий соответствуют определенные изображения. 
        </p>
        <p>
        Откуда вообще в&nbsp;ImageNet возникли такие категории? Таксономия ImageNet опирается на&nbsp;WordNet: базу данных для классификации слов, созданную в&nbsp;1980-х. Именно оттуда в&nbsp;ImageNet попали оскорбительные и&nbsp;субъективные категории. Сейчас категории и&nbsp;изображения, описывающие людей, просто исключены из&nbsp;текущей версии набора данных ImageNet, однако в&nbsp;интернете все еще встречаются старые варианты, а&nbsp;где-то используются алгоритмы, обученные на&nbsp;этой дискриминирующей&nbsp;версии. 
        </p>
        <p>
        Этот&nbsp;кейс приводит нас к&nbsp;выводу о&nbsp;том, что наборы данных, составленные для алгоритмов искусственного интеллекта, несмотря на&nbsp;кажущуюся объективность, являются продуктом конкретных решений. Причем за&nbsp;их&nbsp;принятием стояло ограниченное число людей, которые однажды добавили категории &laquo;клептоман&raquo;, &laquo;гермафродит&raquo;, &laquo;большевик&raquo; и&nbsp;снабдили их&nbsp;иллюстрациями. Это лишь десятки из&nbsp;20&nbsp;тысяч категорий, которые, тем не&nbsp;менее, могут проскользнуть в&nbsp;системы управления беспилотным транспортом или видеонаблюдения и&nbsp;повлиять на&nbsp;действия, совершаемые с&nbsp;опорой на&nbsp;эти технологии.
        </p>
        <p>
        Помимо того, что в&nbsp;наборы данных могут попадать спорные категории, оттуда порой исчезает и&nbsp;необходимая информация. Вот наглядный пример: когда компания Amazon запустила систему рекрутинга на&nbsp;основе искусственного интеллекта в&nbsp;середине 2010-х, обнаружилась странная закономерность&nbsp;&mdash; оценки женщин были ниже. Проблема снова была в&nbsp;данных: алгоритм обучался на&nbsp;резюме, которые приходили в&nbsp;Amazon в&nbsp;течение последних десяти лет. Подавляющее большинство заявок было от&nbsp;кандидатов-мужчин, которых, к&nbsp;тому&nbsp;же, приглашали на&nbsp;работу в&nbsp;компанию чаще, чем женщин. Конечно, основываясь на&nbsp;таких данных, алгоритм Amazon предпочел мужчин. <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G">Отбраковывались</a> резюме, содержащие слова &laquo;женский&raquo;: например, &laquo;капитан женского шахматного клуба&raquo;, или &laquo;выпускницы женских колледжей&raquo;. В&nbsp;итоге, компания отказалась от&nbsp;использования искусственного интеллекта при приеме&nbsp;наработу. 
        </p>
        <p>
        Аналогичные проблемы могут возникнуть при использовании искусственного интеллекта для оценки абитуриентов. Такие системы <a href="https://www.fastcompany.com/90342596/schools-are-quietly-turning-to-ai-to-help-pick-who-gets-in-what-could-go-wrong">внедряют</a>, чтобы избавиться от&nbsp;предубеждений на&nbsp;этапе приемных кампаний. Однако мы&nbsp;уже знаем: чтобы научить алгоритм советовать и&nbsp;предсказывать, мы&nbsp;должны показать ему информацию, на&nbsp;которой он&nbsp;будет делать свои выводы. А&nbsp;если проблема предвзятости при приеме в&nbsp;университет существует, то&nbsp;она отразится и&nbsp;на&nbsp;результатах. В&nbsp;нью-йоркских колледжах несмотря на&nbsp;относительно пропорциональное число заявок на&nbsp;поступление от&nbsp;темнокожих и&nbsp;белых студентов, среди зачисленных подавляющее большинство <a href="https://www.fastcompany.com/90331368/nyc-students-take-aim-at-segregation-by-hacking-an-algorithm">составляют</a> именно белые. Также в&nbsp;США <a href="https://www.wired.com/story/computer-science-graduates-diversity/">число женщин</a>, получающих образование в&nbsp;области компьютерных наук, только снижается: с&nbsp;37% в&nbsp;1980-х до&nbsp;18% в&nbsp;2016&nbsp;году. Такая динамика легко может впоследствие отразиться на&nbsp;логике алгоритма, который будет принимать решения о&nbsp;зачислении в&nbsp;университет.
        </p>

        <figure class="image-container inline-image">
            <img src="https://anatomyof.ai/img/ai-system-map.jpg" alt="">
            <figcaption class="caption" onclick="window.open('https://anatomyof.ai','_blank');">Иллюстрация: Vladan Joler и Kate Crawford</figcaption>
        </figure>

        <h2>Архитектура AI-систем</h2>

        <p>
        Печальная статистика гендерного состава студентов, изучающих компьютерные науки, отражается и&nbsp;на&nbsp;индустрии: <a href="https://pxlnv.com/blog/diversity-of-tech-companies-by-the-numbers-2016/">мужчины занимают</a> от&nbsp;77&nbsp;до&nbsp;83&nbsp;процентов технических позиций в&nbsp;Apple, Facebook, Microsoft, Google и&nbsp;General Electric. Авторы книги &laquo;<a href="https://mitpress.mit.edu/books/smart-wife">The Smart Wife</a>&raquo; Йоланда Стренджерс и&nbsp;Дженни Кеннеди отмечают: среди разработчиков и&nbsp;менеджмента мало не&nbsp;только женщин, но&nbsp;также темнокожих и&nbsp;квир-людей. Из-за этого в&nbsp;IT-компаниях создают целые системы на&nbsp;основе искусственного интеллекта, воспроизводящие и&nbsp;укрепляющие существующие предрассудки.
        </p>
        <p>
        Как&nbsp;следует из&nbsp;названия книги, авторы изучают феномен &laquo;smart wife&raquo;&nbsp;&mdash; так они концептуализируют голосовых ассистентов (Alexa, Siri, Google Home). Речь идет об&nbsp;устройствах и&nbsp;виртуальных агентах, которые по&nbsp;умолчанию наделены стереотипными женскими характеристиками&nbsp;&mdash; в&nbsp;частности, они созданы, чтобы помогать в&nbsp;ведении домашнего хозяйства. Такие ассистенты создаются в&nbsp;основном инженерами-мужчинами&nbsp;&mdash; и&nbsp;мужчины&nbsp;же являются инициаторами установки систем умного дома. Авторы книги выступают за&nbsp;поиск подходов в&nbsp;создании умных ассистентов, которые не&nbsp;будут усиливать существующие гендерные&nbsp;порядки.
        </p>
        <p>
        Дополнительные сложности связаны с&nbsp;самим принципом работы AI-систем: они понимают только числа. Это усложняет работу с&nbsp;такими неустойчивыми категориями как гендерная и&nbsp;сексуальная идентичность. Кроме того, возникает соблазн подобрать конкретные признаки во&nbsp;внешности и&nbsp;поведении, которые&nbsp;бы соответствовали этим искусственно собранным идентичностям. 
        </p>
        <p>
        Кризис разнообразия в&nbsp;данных также негативно отражается на&nbsp;качестве алгоритмов распознавания лиц, например, темнокожих <a href="https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/">людей</a>. Разные компании ищут способы решения этой проблемы. Так, IBM представила в&nbsp;2019 году набор данных &laquo;Разнообразие в&nbsp;лицах&raquo;. Он&nbsp;состоит из&nbsp;десятков тысяч фотографий людей с&nbsp;разным цветом кожи. Правда, к&nbsp;каждому изображению прилагается информация о&nbsp;симметрии лица и&nbsp;измерения черепа. Уже упомянутые выше Кроуфорд и&nbsp;Паглен считают, что такой подход не&nbsp;помогает избавиться от&nbsp;предрассудков, а&nbsp;только усиливает&nbsp;их. Авторы <a href="https://excavating.ai">сравнивают</a> такие данные с&nbsp;практикой краниометрии&nbsp;&mdash; то&nbsp;есть, измерения черепа,&nbsp;&mdash; которая использовалась в&nbsp;19-20&nbsp;веках.
        </p>

        <figure class="image-container inline-image">
            <img src="/img/ai-bias/DiFFace copy.jpg" alt="">
            <figcaption class="caption" onclick="window.open('https://excavating.ai','_blank');">Изображение: IBM's Diversity in Faces</figcaption>
        </figure>

        <h2>Неочевидные признаки</h2>

        <p>
        У&nbsp;систем искусственного интеллекта, в&nbsp;частности компьютерного зрения, есть одна интересная особенность&nbsp;&mdash; они могут самостоятельно находить в&nbsp;изображениях признаки, позволяющие отнести картинку к&nbsp;определенному классу. Программисту не&nbsp;нужно вручную объяснять компьютеру причины, по&nbsp;которым картинки с&nbsp;кошками отличаются от&nbsp;картинок с&nbsp;собаками. Достаточно лишь собрать две папки: одну&nbsp;&mdash; с&nbsp;кошками, другую&nbsp;&mdash; с&nbsp;собаками, а&nbsp;алгоритм сам научится находить признаки и&nbsp;сортировать изображения, которые он&nbsp;прежде не&nbsp;видел. Причем признаки, которые находит машина, часто бывают непонятными для человека, а&nbsp;некоторые мы&nbsp;даже не&nbsp;в&nbsp;состоянии различить.
        </p>
        <p>
        Профессор из&nbsp;Стэнфорда Михал Косински совместно со&nbsp;своим коллегой Йилун Уэнгом в&nbsp;<a href="https://psyarxiv.com/hv28a/">статье</a> 2017 года описал, что алгоритм на&nbsp;основе сверточных нейронных сетей (архитектура нейронных сетей, изначально нацеленная на&nbsp;эффективное распознавание изображений&nbsp;&mdash; <em>прим. сверхновой</em>) может определить сексуальную идентичность человека по&nbsp;его лицу. При этом не&nbsp;существует известных объективных признаков, которые позволяли&nbsp;бы сделать это человеческому глазу. Однако само исследование можно раскритиковать за&nbsp;упрощение категории сексуальной идентичности и&nbsp;ее&nbsp;редукции до&nbsp;&laquo;гомосексуальной&raquo; и&nbsp;&laquo;гетеросексуальной&raquo;. Сама такая ситуация распознавания создает патологизацию: статус аномалии приписывается признакам, которые, на&nbsp;самом деле, никак не&nbsp;связаны с&nbsp;сексуальной идентичностью. Использование такой системы, несмотря на&nbsp;ее&nbsp;ограниченность с&nbsp;точки зрения спектра идентичностей, может поставить в&nbsp;реальную опасность людей в&nbsp;тех странах, где квир-персоны маргинализированы. 
        </p>
        <p>
        В&nbsp;другой <a href="https://arxiv.org/pdf/2107.10356.pdf">статье</a>, вышедшей летом 2021&nbsp;года, описывается, что системы на&nbsp;основе искусственного интеллекта могут распознавать этническую принадлежность пациента по&nbsp;его медицинским снимкам: рентгену и&nbsp;компьютерным томографиям. При этом человек сделать этого не&nbsp;может. Исследователи проверили возможные физиологические признаки этнической принадлежности и&nbsp;оказалось, что для&nbsp;ИИ они не&nbsp;играют никакой роли при прогнозировании. Кроме того, точность распознавания остается высокой даже на&nbsp;сильно зашумленных снимках или если часть лица скрыта. Уже известны <a href="https://www.nytimes.com/2019/11/19/technology/artificial-intelligence-bias.html">случаи</a>, когда искусственный интеллект научился определять переломы на&nbsp;рентгеновских снимках не&nbsp;глядя на&nbsp;саму кость, а&nbsp;определяя, в&nbsp;какой больнице был сделан снимок: это можно сделать из-за определенных характеристик при настройке аппарата. Все эти примеры могут создавать сложности при работе с&nbsp;пациентами.
        </p>

        <h2>Что&nbsp;с&nbsp;этим делать?</h2>

        <p>
        Предрассудки в&nbsp;алгоритмах искусственного интеллекта изучаются исследователями и&nbsp;практиками. Создаются различные библиотеки и&nbsp;инструменты для справедливого (Fairness) машинного обучения, которые позволяют убедиться, что на&nbsp;работу алгоритма не&nbsp;влияют пол, этничность, сексуальная идентичность, вероисповедание и&nbsp;другие признаки. Например, библиотеки <a href="https://aif360.mybluemix.net">AI&nbsp;Fairness 360</a> от&nbsp;IBM и&nbsp;<a href="https://fairlearn.org">Fairlearn</a>. Они показывают, какие признаки могут повлиять на&nbsp;объективность результата. Однако такой подход, по&nbsp;сути, изолирующий отдельные признаки, не&nbsp;всегда актуален. Во-первых, уменьшение размеров набора данных влияет на&nbsp;его точность. Во-вторых, могут существовать другие признаки, которые коррелируют с&nbsp;теми, что чаще приводят к&nbsp;предрассудкам, но&nbsp;не&nbsp;подлежат исключению: информация о&nbsp;месте жительства, семейном положении, образовании и&nbsp;многом другом. Кроме того, в&nbsp;уже упомянутой <a href="https://www.yalelawjournal.org/pdf/Mayson_p5g2tz2m.pdf">статье</a> Сары Мэйсон говорится, что исключение отдельных признаков из&nbsp;данных, в&nbsp;частности этничности, не&nbsp;решает проблему: игнорирование этнической принадлежности означает игнорирование расизма. Знание об&nbsp;институциональном расизме как раз крайне важно учитывать при проектировании систем&nbsp;ИИ.
        </p>
        <p>
        Вряд&nbsp;ли можно раз и&nbsp;навсегда избавиться от&nbsp;предрассудков в&nbsp;системах искусственного интеллекта, ведь людей без предрассудков также не&nbsp;существует. Меньшее и&nbsp;самое реалистичное, что мы&nbsp;можем сделать для решения этой проблемы&nbsp;&mdash; включать в&nbsp;разработку систем на&nbsp;основе искусственного интеллекта группы людей, которые меньше всего в&nbsp;них представлены. На&nbsp;это нацелены такие инициативы как <a href="https://blackinai.github.io/#/">Black in&nbsp;AI</a> и&nbsp;<a href="https://sites.google.com/view/queer-in-ai/">Queer in&nbsp;AI</a>, работающие над образовательными проектами и&nbsp;поиском грантов для своих участников.
        </p>
        <p>
        Сейчас для многих людей искусственный интеллект является ничем иным, как черной коробкой, при этом совершенно объективной и&nbsp;непредвзятой. И&nbsp;как ни&nbsp;парадоксально, чем более распространенной становится технология, тем меньше мы&nbsp;знаем о&nbsp;принципах ее&nbsp;работы. Крайне важно уже само осознание того, что алгоритмы и&nbsp;данные субъективны и&nbsp;могут быть носителями предрассудков. А&nbsp;понимание принципов работы искусственного интеллекта позволяет сэкономить время, которое требуется для создания более инклюзивных и&nbsp;справедливых систем.
        </p>

    </article>

    <div class="headline-block">
        <div class="byline-container" onclick="window.open('https://t.me/smlldt','_blank');">
            <div class="author-pic">
                <img style="height: 100%;" src="/img/ai-bias/denis.JPG" alt="">
            </div>
            <div class="author-text">
                <address class="byline"><p rel="author" href="https://t.me/smlldt">Денис Протопопов</p></address>
                <div class="byline-desc">Художник и исследователь, работающий на пересечении технологий, медиа, визуальных и перформативных искусств</div>
                <div class="byline-link"> <a href="https://t.me/smlldt">@smlldt</a></div>
            </div>
        </div>
    </div>

    <div class="links">
        <a href="https://www.facebook.com/supernovafuture/" target="_blank" id="fb">Facebook</a>
        <a href="https://www.instagram.com/supernova_future/" target="_blank">Instagram</a>
        <a href="https://www.vk.com/supernovafuture/" target="_blank">VK</a>
        <a href="https://www.twitter.com/supernovafuture/" target="_blank">Twitter</a>
        <a href="https://www.t.me/supernovafuture/" target="_blank">Telegram</a>
        <a href="https://supernovamedia.substack.com/p/coming-soon" target="_blank" style="margin-right: 0px !important" id="twitter">Рассылка</a>
    </div>
    </div>
    <div class="footer">
        <div class="footer-text" id="rating">18 +</div>
        <div class="footer-text" id="sn-footer">
            <a href="/about.html">сверхновая</a>
        </div>
        <div class="footer-text" id="plausable">
            <a href="https://plausible.io/supernova.is" target="_blank">Мы&nbsp;используем ответственную аналитику от&nbsp;Plausible</a>
        </div>
        <div class="footer-text">
            <a href="mailto:superfuturenova@gmail.com">Связаться с нами</a>
        </div>
    </div>

    <!-- JS onlick method courtesy of Chris Kraack -->
    <!-- JS link method courtesy of Sergey Surganov and Pranav C Balan -->
    <script type="text/javascript">
        Array.prototype.forEach.call(document.querySelectorAll('.text a, .byline-link a'), function (link) { link.target = "_blank" });

        console.log("Не читайте этот код, сохраните себе нервы (╯°□°）╯︵ ┻━┻");

    </script>
</body>
</html>